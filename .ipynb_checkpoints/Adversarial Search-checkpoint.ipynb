{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   Adversarial Search in Game Playing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical game theory, a branch of economics, views any multiagent environment as a game, provided that the impact of each agent on the other is 'significant' regardless of whether they're competitive or cooperative. The type of games we usually study in Artificial Intelligence are special kind of games which are referred to as __zero-sum games__. These type of games are deterministic, turn-taking, two-player, of __perfect information__. When we say sero-sum, we mean a game situation in which each participant's gain or loss of utility is exactly balanced by the losses or gains of the utility of the other participants. When we say perfect information, we mean that we have complete knowledge of all the states in the future, actions possible from each state, and the value achieved at the end of the game. For example, tic-tac-toe, Go, and chess are an example of a zero-sum game of perfect information. On the other hand, games like Poker are called __imperfect information__ games, because each player's cards are hidden from other players and therefore it's not possible to know which states are possible in the future. \n",
    "\n",
    "\n",
    "Before we start discussing the adversarial search techniques in game-playing, we need to define what does game playing mean. According to Peter Norvig's __Artificial Intelligence - A Modern Approach__, game-playing can formally be defined as a kind of search problem with the following elements:\n",
    "\n",
    "> - S$_{0}$: The __initial state__, which defines how the game is set up at the start,\n",
    "> - Player(s): Defines which players has the turn in a state,\n",
    "> - Actions(s): Returns the set of legal moves at state s,\n",
    "> - Result(s, a): The __transition model__, which defines the result of a move,\n",
    "> - Terminal-Test(s): A __terminal test__, which is true when the game is over and false otherwise. States where the game has ended are called __terminal states__.\n",
    "> - Utility(s, p): Returns the __value__ of the game for player p at the terminal state s.\n",
    "\n",
    "\n",
    "Having defined what game-playing is, we need to understand the set-up on which (adversarial) search algorithms can work. We usually use a Tree data structure to formalize the game setting. The initial state, Actions function, and Result function define a game tree for a given game. In a game tree, each node corresponds to game states and edges corresponds to possible action from that state (node). For example, the tree search for tic-tac-toe would look like this: \n",
    "\n",
    "![title](images/tictactoe.png)\n",
    "\n",
    "For tic-tac-toe, the number of terminal game states is upper-bounded by 9! = 362,880 terminal states, but for games like Chess and Go, the number of nodes in a game tree search is extremely high (higher than 10$^{40}$). Now let's move on to how we define optimal moves in a game.\n",
    "\n",
    "### Optimal Move Strategies\n",
    "\n",
    "Roughly speaking, an optimal game playing strategy leads to outcomes at least as good as any other strategy when playing against an infallible opponent. We will discuss a few major optimal strategies algorithms like MiniMax, Alpha-Beta Pruning, and the Monte-Carlo Tree Search. Since the main focus of this post is explain a major component of AlphaZero, we will not go in depth on MiniMax and Alpha-Beta pruning, but just provide enough information for the reader to have an idea of what these algorithms aim to achieve.\n",
    "\n",
    "#### MiniMax Algorithm:\n",
    "\n",
    "To understand MiniMax algorithm, let's first define what a __minimax value__ and a __minimax decision__ is. Since we're only considering two-players games like tic-tac-toe, chess, Go etc, let's call one player MAX and the other MIN. Take a look at the game search tree below. \n",
    "\n",
    "![title](images/minmax.png)\n",
    "\n",
    "Here, assuming MAX takes a turn at the initial state S$_{0}$ (root node), it would select the move that leads to its child state with the highest _value_ (state B from action a$_{1}$). Now MIN will take a turn at the next level (depth = 1), and it would select the move that leads to its child state with the lowest _value_ (state from action b$_{1}$) and so on. Therefore, we define the __minimax value__ as the max value state available for MAX and min value state available for MIN. But how do we decide these values? These values could be as simple as the sum of wins from a given state. For example, in tic tac toe, the terminal nodes have values -1 (min wins), 0 (draw), or +1 (max wins). So the values would be -1, 0, or +1. Formally, __minimax value__ is defined as follows:\n",
    "\n",
    "![title](images/minmaxvalue.png)\n",
    "\n",
    "And __minimax decision__ is defined as a decision (move) based on __minimax value__. \n",
    "\n",
    "The MiniMax Algorithm computes the minimax decision from the current state. It uses a simple recursive computation of the minimax values of each successor state, directly implementing the defining equations. The recursion proceeds all the way down to the leaves of the tree, and then the minimax values are backed up through the tree as the recursion\n",
    "unwinds. The minimax algorithm performs a complete depth-first search of the game tree. If the maximum depth of the game tree is m, and there are b legal moves at each point, then __the time complexity of the minimax algorithm is O(b$^{m}$)__ and __the space complexity is O(bm)__. Though minimax is provable an optimal game strategy, we can see how it becomes computatationally unfeasible to carry out on game trees with high depth and high branching factor. This is why we would need improvements on minimax algorithm, which we'll discuss later in this post. Here's the pseudo code and a basic implementation of the minimax algorithm. \n",
    "\n",
    "![title](images/minmaxalg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniMax:\n",
    "    def __init__(self, game_tree):\n",
    "        self.game_tree = game_tree\n",
    "        self.root = game_tree.root\n",
    "        self.currentNode = None \n",
    "        self.successors = []\n",
    "\n",
    "    def minimax_value(self, node):\n",
    "        \"\"\"Takes as input the current state and returns the minimax decision\"\"\"\n",
    "        best_val = self.max_value(node) \n",
    "\n",
    "        successors = self.getSuccessors(node)\n",
    "        print(\"Running MiniMax at node: \" + str(best_val))\n",
    "        best_move = None\n",
    "        for move in successors:\n",
    "            if move.value == best_val:\n",
    "                best_move = move\n",
    "                break\n",
    "        return best_move\n",
    "\n",
    "\n",
    "    def max_value(self, node):\n",
    "        \"\"\"Takes as input the current state and returns the state with the max value\"\"\"\n",
    "        \n",
    "        print(\"Checking for MAX at node: \" + node.Name)\n",
    "        if self.isTerminal(node):\n",
    "            return self.getUtility(node)\n",
    "        \n",
    "        max_value = -(float('inf')) #set the initial max_value to negative infinity (update when larger value found)\n",
    "        successors_states = self.getSuccessors(node)\n",
    "        for state in successors_states:\n",
    "            max_value = max(max_value, self.min_value(state))\n",
    "        return max_value\n",
    "\n",
    "    def min_value(self, node):\n",
    "        \"\"\"Takes as input the current state and returns the state with the min value\"\"\"\n",
    "        print(\"Checking for MIN at node: \" + node.Name)\n",
    "        if self.isTerminal(node):\n",
    "            return self.getUtility(node)\n",
    "\n",
    "        min_value = float('inf') #set the initial min_value to positive infinity (update when larger value found)\n",
    "        successor_states = self.getSuccessors(node)\n",
    "        for state in successor_states:\n",
    "            min_value = min(min_value, self.max_value(state))\n",
    "        return min_value\n",
    "\n",
    "    \n",
    "    \"\"\"Helper Functions\"\"\"\n",
    "    \n",
    "    def getSuccessors(self, node):\n",
    "        \"\"\"Takes as input the current state and returns the children states\"\"\"\n",
    "        assert node is not None\n",
    "        return node.children\n",
    "\n",
    "    def isTerminal(self, node):\n",
    "        \"\"\"Takes as input the current state and returns True if it's a terminal state\"\"\"\n",
    "        assert node is not None\n",
    "        return len(node.children) == 0\n",
    "\n",
    "    def getUtility(self, node):\n",
    "        \"\"\"Takes as input the current state and returns its value\"\"\"\n",
    "        assert node is not None\n",
    "        return node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha-Beta Pruning:\n",
    "\n",
    "The problem with minimax strategy is that the number of game states it examines for evaluation grows exponentially in terms of the depth of the tree. So even for games like tic-tac-toe, the upper bound for the number of state evaluations would be O(9$^{9}$) because the maximum number of moves are 9 (at root S$_{0}$) and maximum depth is also 9. Unfortunately, we can't do better than the exponent since we're essentially performing a depth-first search. However, we can cut it in half. The key is that it is possible to compute the minimax strategy without looking at every state. The technique to achieve so is called __Alpha Beta Pruning__. Alpha and Beta are the two variable that indicate at each state the best (in term of both max and min) we can do and these help us 'prune' the game tree search that we know is not going to affect the minimax decision. \n",
    "\n",
    "More formally,\n",
    "- $\\alpha$ = the value of the best (highest value) choice we have found so far at any point along the path for MAX\n",
    "- $\\beta$ = the value of the best (lowest value) choice we have found so far at any point along the path for MIN\n",
    "\n",
    "The general strategy is as follows: consider a node n somewhere in the tree, such that player has choice of moving to that node. If the player has a better choice m either at the parent node of n or at any choice point further up, then n will never be reached in actual play. So once we have found enough about n by evaluating some if its descendants, we can prune it. Alpha-Beta Pruning updates $\\alpha$ and $\\beta$ as it goes along and prunes the remaining branches at a node as soon as the value of the current node is known to be worse than the current $\\alpha$ or $\\beta$ for MAX or MIN respectively. \n",
    "\n",
    "Following is the pseudocode and a basic implementation of Alpha-Beta Pruning. \n",
    "\n",
    "![title](images/alphabeta.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBeta:\n",
    "    def __init__(self, game_tree):\n",
    "        self.game_tree = game_tree\n",
    "        self.root = game_tree.root\n",
    "\n",
    "    def alpha_beta_search(self, node):\n",
    "        best_val = -(float('inf'))\n",
    "        beta = -(float('inf'))\n",
    "\n",
    "        successors = self.getSuccessors(node)\n",
    "        best_state = None\n",
    "        for state in successors:\n",
    "            value = self.min_value(state, best_val, beta)\n",
    "            if value > best_val:\n",
    "                best_val = value\n",
    "                best_state = state\n",
    "        print(\"AlphaBeta: Utility at Root Node: \" + str(best_val))\n",
    "        return best_state\n",
    "\n",
    "    def max_value(self, node, alpha, beta):\n",
    "        print(\"Check for Max at node: \" + node.Name)\n",
    "        if self.isTerminal(node):\n",
    "            return self.getUtility(node)\n",
    "        value = -(float('inf'))\n",
    "\n",
    "        successors = self.getSuccessors(node)\n",
    "        for state in successors:\n",
    "            value = max(value, self.min_value(state, alpha, beta))\n",
    "            if value >= beta:\n",
    "                return value\n",
    "            alpha = max(alpha, value)\n",
    "        return value\n",
    "\n",
    "    def min_value(self, node, alpha, beta):\n",
    "        print(\"Check for Min at node: \" + node.Name)\n",
    "        if self.isTerminal(node):\n",
    "            return self.getUtility(node)\n",
    "        value = float('inf')\n",
    "\n",
    "        successors = self.getSuccessors(node)\n",
    "        for state in successors:\n",
    "            value = min(value, self.max_value(state, alpha, beta))\n",
    "            if value <= alpha:\n",
    "                return value\n",
    "            beta = min(beta, value)\n",
    "\n",
    "        return value\n",
    "\n",
    "    \"\"\"Helper Functions\"\"\"\n",
    "\n",
    "    def getSuccessors(self, node):\n",
    "        assert node is not None\n",
    "        return node.children\n",
    "\n",
    "    def isTerminal(self, node):\n",
    "        assert node is not None\n",
    "        return len(node.children) == 0\n",
    "\n",
    "    def getUtility(self, node):\n",
    "        assert node is not None\n",
    "        return node.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha Beta Pruning is definitely a poweful algorithm that achieves optimal game playing strategy. IBM's Deep Blue that defeated a chess world champion in 1996 was designed using Alpha-Beta search. For games like Go, the game tree complexity is immensely large (10$^{170}$ legal moves on a 19x19 board). Therefore, it becomes computationally infeasible to run Alpha-Beta search. Just to have an abstract idea of how the game search tree grows for Go, take a look at the image below. \n",
    "\n",
    "![title](images/gotree.gif)\n",
    "\n",
    "Due to the high branching factor in the game tree for Go, Alpha-Beta Pruning does not stand as an effective choice. As a way around the high computattion cost attached with games like Go, we can do two things:\n",
    "\n",
    "> - Use a heuristic function, which is a function that gives an estimate of the value of a state of the game. Instead of considering all the moves in a game, you can just consider moves out to some finite distance ahead, and then use the value of the heuristic to judge the value of the states you reached.\n",
    "\n",
    "> - Use Rollouts. At a given state, just randomly simulate the game by taking turns for both the players until you reach a terminal state. The belief is that as the number of rollouts increase, the sample average win rate (no. wins / no. rollouts) will converge to the expected win rate from that state.\n",
    "\n",
    "Since, a lot of complex games like Go lack any definitive heuristic are based more on human intuition, it is hard to come up with a single heuristic function that would evaluate the game states for us. This leads us to focus on the second option of using Rollouts. In particular, we will discuss another extremely powerful game tree search strategy - __the Monte Carlo Tree Search (MCTS)__. MCTS is the algorithm of choice for AlphaZero and hence we will dive deeper into this algorithm to understand why it is a better choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
