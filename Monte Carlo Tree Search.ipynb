{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook is dedicated to the explanation of __Monte Carlo Tree Search__ algorithm, which is a key ingredient in the training of AlphaZero program. To explain the concepts involved in MCTS, we will use an example of Tic Tac Toe, as games like Go and Chess have really huge search trees to illustrate the concepts on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Tree Search works on types of games I described before in the Adversarial Search notebook - zero-sum games. These are the deterministic games of perfect information. When we say that a game is of perfect information, it means that we can sketch out the entire game search tree to look at all the possible game states and actions. \n",
    "\n",
    "Classic Game Playing strategies, like MiniMax and Alpha-Beta, will simply traverse this game tree and choose a node that they think is most likely to lead them to victory. As we had stated earlier, this technique is provably optimal but terrible when it comes to time complexity for games with high branching factor (like Go). This is where Monte Carlo Tree Search comes in. \n",
    "\n",
    "Since, in MCTS, we begin with an empty search tree and expand it by following the MCTS algorithm (will be dicussed shortly), the tree growth is asymmetric. This provides a huge advantage over MiniMax, which needs a full game tree, because ulike MiniMax, MCTS can be configured to stop after a desired amount of time (# of steps) and we can still select a sufficiently optimal move based on the partially constructed search tree. \n",
    "\n",
    "Lets's now see how Monte-Carlo Tree Search Algorithm works. \n",
    "\n",
    "The key idea is that in MCTS, we keep some stats for each node that help us decide which action to take at a give state. Remember, nodes in tree represent the game states and edges represent the action possible. So an edge e from node A to node B would mean that taking action e from game state A would result in game state B. Now what are the stats that we keep for the game states? They are:\n",
    "> - $Q(s,a)$ = it is the win-ratio for a given state (# simulated wins / # simulated games)\n",
    "> - $N$ = total number of simulations that have occured after $i^{th}$ iteration\n",
    "> - $n$ = total number of sumilations for a given node after $i^{th}$ iteration\n",
    "\n",
    "And how do we use these stats to help us navigate through the game search tree? This is where the key of MCTS is! These stats are used to construct a score called the __Upper Confidence Bound__ for each state. At a given state, if we have multiple child nodes (games states resulting from possible moves) and we have to select one, we select the one with the highest Upper Confidence Bound. \n",
    "\n",
    "Let's now see what this cryptic term is. \n",
    "\n",
    "### Upper Confidence Bound\n",
    "\n",
    "The formula for UCB is as follows: \n",
    ">>> $UCB(s, a) = Q(s, a) + c.\\sqrt{\\ln(N) / n}$\n",
    "\n",
    "The first term in the equation above is the __exploitation term__ and the second term is the __exploration term__. Since the first term is simply the number of simulated games that lead to win / number of total simulated games, this term will be high for the states that lead to more wins during the simulations. So this term essentially is the mean value of a given state. The second term is more interesting -- it creates a upper confidence bound on the mean value score of each state and __measure the ucertainty in the measure of state's value__. Let's see how it works.\n",
    "\n",
    "In $c.\\sqrt{\\ln(N) /n}$, $c$ is the exploration coefficient. If c = 1, both exploitation and exploration are given equal weightage. If a given state $s_{0}$ (or action leading to it) is chosen many times, the uncertainty goes down (exploration term becomes small) as the $n$ in the denominator is incremented. On the other hand, if any other state is chosen, the uncertainty for $s_{0}$ value goes up as the $N$ in the numerator is incremented while $n$ in the denominator stays constant. Also, we use the natural logartihm to make the increase in uncertainty become smaller over time, but also unbounded. For more in depth explanation, please refer to __Reinforcement Learning: An Introduction by Sutton__.\n",
    "\n",
    "Upper Confidence Bound (UCB) is used in MCTC to determine which move to take. However, there are altogether 4 key steps in the MCTS algorithm - __Selection, Expansion, Rollout, and Update__. Let's see what each of these terms means.\n",
    "\n",
    "> 1) __Selection__: In this part, we begin at the root node of the tree, __R__ and we select the successive child node with the highest UCB value until we reach a node with no more children, __L__. \n",
    "\n",
    "<img src=\"images/NN/sel.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "> 2) __Expansion__: In this part, we check if __L__ is not a terminal state. If it's not, we create all states possible from actions available at L and add them to the tree. Then we select the first child node, __C__.\n",
    "\n",
    "<img src=\"images/NN/exp.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "> 3) __Rollout__: In this part, we generate a random playout from node __C__ until we reach the end of the game, where the result is either win, draw, or loss. \n",
    "\n",
    "<img src=\"images/NN/roll.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "> 4) __Update__: In this part, we use the result obtained at the end of the rollout (win/draw/loss) and propagate the result back up from __C__ to __R__, hence updating the relevant values in that trajectory. \n",
    "\n",
    "<img src=\"images/NN/upd.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The four steps above are part of one iteration, so in Monte-Carlo Tree Search Algorithm, we perform a fixed number of iterations (starting from the root node each time) and keep expanding and updating the tree. The tree grows asymmetrically. After the fixed number of iterations has elapsed, we will select the action from root node $s_{0}$ that has the highest UCB score (calculates using the updated stats in the tree). The resultant state (child node) will become the root node now and we will repeat the fixed number of iterations to pick the next move by the opponent. \n",
    "\n",
    "__The beauty of MCTS(UCB) is that, due to its asymmetrical nature, the tree selection and growth gradually converges to better moves. At the end, you get the child node with the highest number of simulations and thatâ€™s your best move according to MCTS.__\n",
    "\n",
    "Before we code the algorithm up, let's state the benefits and the drawbacks of MCTS.\n",
    "\n",
    "__Benefits__: \n",
    "> - __Aheuristic__: MCTS does not require domain knowledge for any game beyod just is rules. This means that the MCTS code can be reused for many different games with little modifications.\n",
    "> - __Asymmetric__: MCTS performs asymmetric tree growth that adapts to the topology of the search space. The algorithm visits more interesting nodes more often, and focusses its search time in more relevant parts of the tree.\n",
    "> - __anytime__: The algorithm could be stopped at any iteration number and we will have the current optimal policy.\n",
    "\n",
    "__Drawbacks__:\n",
    "> - __Speed__: MCTS search can take many iterations to converge to a good solution. We will see that this is one of the reasons we use an improved MCTS in AlphZero (use Neural Networks instead of random rollouts).\n",
    "> - __Playing Strength__: MCTS can sometimes fail to find reasonable moves for even games of medium complexity within a reasonable number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a high-level view of the MCTS algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MCTS(s, game):\n",
    "    \"\"\"Input: s ~ state\n",
    "       game = specific game\"\"\"\n",
    "\n",
    "    if game.gameEnded(s):\n",
    "        return -game.gameReward(s)\n",
    "\n",
    "    # Expand\n",
    "    if s not in visited:\n",
    "        visited.add(s)\n",
    "        v = rollout(s, game) #Rollout\n",
    "        return -v\n",
    "\n",
    "    max_ucb, best_action = -float(\"inf\"), -1\n",
    "    \n",
    "    # Select\n",
    "    for a in range(game.getValidActions(s)):\n",
    "        ucb = Q[s][a] + (c * sqrt(np.log(sum(N[s])/N[s][a])))\n",
    "        #c = exploration constant\n",
    "        #N = dictionary of states as keys mapping to list containing number of simulations corresponding to each of its action\n",
    "        #Q = disctionary with states as keys mapping to list containing values corresponding to possible actions\n",
    "\n",
    "        if ucb > max_ucb:\n",
    "            max_ucb = ucb\n",
    "            best_action = a\n",
    "\n",
    "    a = best_action\n",
    "\n",
    "    s_next = game.nextState(s, a)\n",
    "\n",
    "    v = MCTS(s_next, game)\n",
    "\n",
    "\n",
    "    # Update\n",
    "    Q[s][a] = (N[s][a]*Q[s][a] + v)/(N[s][a] + 1)\n",
    "\n",
    "    N[s][a] += 1\n",
    "\n",
    "    return -v\n",
    "\n",
    "def rollout(s, game):\n",
    "    while True:\n",
    "        if teminalState(s, game):\n",
    "            return -v\n",
    "        val_act = game.getValidActions(s)\n",
    "        a = np.random.randint(0,len(val_act))\n",
    "        action = val_act[a]\n",
    "        simulate(action, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
